"""
Script d'entra√Ænement du mod√®le de Machine Learning
Compare plusieurs algorithmes et choisit le meilleur automatiquement
"""

import os
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
import numpy as np
import pandas as pd
import time
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from pathlib import Path


def calculate_roc_data(model, model_name, X_test, y_test, classes):
    """
    Calcule les donn√©es ROC pour un mod√®le
    """
    # Binariser les labels
    y_test_bin = label_binarize(y_test, classes=classes)
    
    # Obtenir les probabilit√©s de pr√©diction
    if hasattr(model, 'predict_proba'):
        y_score = model.predict_proba(X_test)
    elif hasattr(model, 'decision_function'):
        y_score = model.decision_function(X_test)
        from scipy.special import softmax
        y_score = softmax(y_score, axis=1)
    else:
        return None
    
    # Calculer micro-average ROC
    fpr_micro, tpr_micro, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())
    roc_auc_micro = auc(fpr_micro, tpr_micro)
    
    return {
        'fpr': fpr_micro,
        'tpr': tpr_micro,
        'auc': roc_auc_micro,
        'name': model_name
    }


def plot_all_roc_curves(roc_data_list, output_dir='ml/results'):
    """
    G√©n√®re un graphique unique avec toutes les courbes ROC et sauvegarde en PNG
    
    Args:
        roc_data_list: Liste des donn√©es ROC pour chaque mod√®le
        output_dir: Dossier de sortie
    """
    # Cr√©er le dossier de sortie
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Cr√©er la figure Plotly
    fig = go.Figure()
    
    # Couleurs modernes pour chaque mod√®le
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
    
    # Ajouter les courbes ROC pour chaque mod√®le
    for i, roc_data in enumerate(roc_data_list):
        if roc_data is None:
            continue
            
        fig.add_trace(go.Scatter(
            x=roc_data['fpr'],
            y=roc_data['tpr'],
            mode='lines',
            name=f"{roc_data['name']} (AUC = {roc_data['auc']:.3f})",
            line=dict(color=colors[i % len(colors)], width=3)
        ))
    
    # Ajouter la ligne de r√©f√©rence (al√©atoire)
    fig.add_trace(go.Scatter(
        x=[0, 1],
        y=[0, 1],
        mode='lines',
        name='Al√©atoire (AUC = 0.500)',
        line=dict(color='gray', width=2, dash='dash')
    ))
    
    # Personnaliser le layout
    fig.update_layout(
        title=dict(
            text='<b>Comparaison des Courbes ROC - Tous les Mod√®les</b><br>' +
                 '<sub>Micro-Average ROC pour chaque algorithme</sub>',
            x=0.5,
            xanchor='center',
            font=dict(size=20, color='#2c3e50')
        ),
        xaxis=dict(
            title='<b>Taux de Faux Positifs (FPR)</b>',
            title_font=dict(size=14, color='#34495e'),
            gridcolor='rgba(200, 200, 200, 0.3)',
            range=[0, 1]
        ),
        yaxis=dict(
            title='<b>Taux de Vrais Positifs (TPR)</b>',
            title_font=dict(size=14, color='#34495e'),
            gridcolor='rgba(200, 200, 200, 0.3)',
            range=[0, 1.05]
        ),
        plot_bgcolor='white',
        paper_bgcolor='white',
        legend=dict(
            x=0.98,
            y=0.02,
            xanchor='right',
            yanchor='bottom',
            bgcolor='rgba(255, 255, 255, 0.9)',
            bordercolor='rgba(0, 0, 0, 0.2)',
            borderwidth=1,
            font=dict(size=12)
        ),
        width=1200,
        height=900
    )
    
    # Sauvegarder en PNG
    output_path = Path(output_dir) / "roc_curves_comparison.png"
    fig.write_image(str(output_path), format='png', scale=2)
    
    print(f"\nüìä Courbe ROC comparative sauvegard√©e: {output_path}")
    print(f"   R√©solution: 2400x1800 pixels (haute qualit√©)")


def prepare_training_data(csv_path='ml/training_data.csv'):
    """
    Charge les donn√©es d'entra√Ænement depuis un fichier CSV
    
    Args:
        csv_path: Chemin vers le fichier CSV contenant les donn√©es
        
    Returns:
        Tuple (X, y) avec les textes et les labels
    """
    print(f"üìÇ Chargement des donn√©es depuis {csv_path}...")
    
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Le fichier {csv_path} n'existe pas!")
    
    # Charger le CSV
    df = pd.read_csv(csv_path, encoding='utf-8')
    
    print(f"‚úÖ Donn√©es charg√©es: {len(df)} exemples")
    print(f"üìä R√©partition par cat√©gorie:")
    print(df['category'].value_counts())
    
    X = df['text'].tolist()
    y = df['category'].tolist()
    
    return X, y

def train_model():
    """
    Entra√Æne plusieurs mod√®les de classification et choisit le meilleur
    """
    print("üöÄ D√©marrage de l'entra√Ænement et comparaison des mod√®les ML...")
    print("="*80)
    
    # Pr√©parer les donn√©es
    X, y = prepare_training_data()
    
    print(f"\nüìä Nombre total d'exemples: {len(X)}")
    print(f"üìä Cat√©gories: {set(y)}")
    
    # Diviser en ensembles d'entra√Ænement et de test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"üìö Ensemble d'entra√Ænement: {len(X_train)} exemples")
    print(f"üß™ Ensemble de test: {len(X_test)} exemples")
    
    # Cr√©er le vectorizer TF-IDF
    print("\nüîß Cr√©ation du vectorizer TF-IDF...")
    vectorizer = TfidfVectorizer(
        max_features=5000,        # Augment√© pour capturer plus de patterns
        ngram_range=(1, 3),       # Unigrammes, bigrammes et trigrammes
        min_df=2,                 # Fr√©quence minimale
        max_df=0.7,               # Fr√©quence maximale
        strip_accents='unicode',  # Retirer les accents
        lowercase=True,           # Convertir en minuscules
        sublinear_tf=True         # √âchelle logarithmique pour TF
    )
    
    # Transformer les textes en features TF-IDF
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)
    
    print(f"‚úÖ Vectorisation termin√©e: {X_train_tfidf.shape[1]} features cr√©√©es")
    
    # D√©finir les mod√®les √† tester
    models = {
        "Naive Bayes (Multinomial)": MultinomialNB(alpha=0.1),
        "Logistic Regression": LogisticRegression(max_iter=1000, C=10, random_state=42),
        "Support Vector Machine (Linear)": LinearSVC(C=1.0, max_iter=2000, random_state=42),
        "Random Forest": RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42)
    }
    
    print("\n" + "="*80)
    print("ü§ñ ENTRA√éNEMENT ET COMPARAISON DES MOD√àLES")
    print("="*80)
    
    results = {}
    roc_data_list = []  # Pour stocker les donn√©es ROC de tous les mod√®les
    best_model = None
    best_model_name = None
    best_accuracy = 0
    
    # Entra√Æner et √©valuer chaque mod√®le
    for model_name, model in models.items():
        print(f"\n{'='*80}")
        print(f"üìä Mod√®le: {model_name}")
        print(f"{'='*80}")
        
        # Mesurer le temps d'entra√Ænement
        start_time = time.time()
        model.fit(X_train_tfidf, y_train)
        train_time = time.time() - start_time
        
        # Pr√©dictions sur TRAIN et TEST
        y_train_pred = model.predict(X_train_tfidf)
        y_test_pred = model.predict(X_test_tfidf)
        
        # Calculer les pr√©cisions
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        
        # D√©tection d'overfitting
        overfitting_gap = train_accuracy - test_accuracy
        is_overfitting = overfitting_gap > 0.10  # Si diff√©rence > 10%
        
        # Cross-validation (5-fold)
        cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='accuracy')
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        
        # Stocker les r√©sultats
        results[model_name] = {
            'model': model,
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'overfitting_gap': overfitting_gap,
            'is_overfitting': is_overfitting,
            'cv_mean': cv_mean,
            'cv_std': cv_std,
            'train_time': train_time,
            'y_pred': y_test_pred
        }
        
        # Afficher les r√©sultats
        print(f"‚è±Ô∏è  Temps d'entra√Ænement: {train_time:.2f}s")
        print(f"üéØ Pr√©cision TRAIN: {train_accuracy * 100:.2f}%")
        print(f"üéØ Pr√©cision TEST:  {test_accuracy * 100:.2f}%")
        print(f"üìä √âcart Train-Test: {overfitting_gap * 100:.2f}%")
        
        # Avertissement overfitting
        if is_overfitting:
            print(f"‚ö†Ô∏è  OVERFITTING D√âTECT√â! (√©cart > 10%)")
        else:
            print(f"‚úÖ Pas d'overfitting (√©cart < 10%)")
        
        print(f"üìà Cross-validation (5-fold): {cv_mean * 100:.2f}% (¬±{cv_std * 100:.2f}%)")
        
        print(f"\nüìä Rapport de classification (TEST):")
        print(classification_report(y_test, y_test_pred, zero_division=0))
        
        # Calculer les donn√©es ROC (sans g√©n√©rer de graphique)
        roc_data = calculate_roc_data(model, model_name, X_test_tfidf, y_test, 
                                      classes=sorted(set(y_test)))
        if roc_data:
            roc_data_list.append(roc_data)
            results[model_name]['roc_auc_micro'] = roc_data['auc']
        
        # Mettre √† jour le meilleur mod√®le (bas√© sur test_accuracy)
        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            best_model = model
            best_model_name = model_name
    
    # G√©n√©rer le graphique ROC comparatif unique
    print("\n" + "="*80)
    print("üìà G√âN√âRATION DU GRAPHIQUE ROC COMPARATIF")
    print("="*80)
    plot_all_roc_curves(roc_data_list, output_dir='ml/results')
    
    # Afficher le r√©capitulatif
    print("\n" + "="*80)
    print("üìä R√âCAPITULATIF DES PERFORMANCES")
    print("="*80)
    
    # Trier par pr√©cision TEST
    sorted_results = sorted(results.items(), key=lambda x: x[1]['test_accuracy'], reverse=True)
    
    print(f"\n{'Rang':<5} {'Mod√®le':<40} {'Train':<10} {'Test':<10} {'AUC':<10} {'√âcart':<10} {'Status'}")
    print("-" * 105)
    
    for rank, (name, result) in enumerate(sorted_results, 1):
        medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else "  "
        overfitting_icon = "‚ö†Ô∏è" if result['is_overfitting'] else "‚úÖ"
        auc_micro = result.get('roc_auc_micro', 0)
        print(f"{medal} {rank:<3} {name:<40} {result['train_accuracy']*100:>6.2f}%  "
              f"{result['test_accuracy']*100:>6.2f}%  {auc_micro:>6.3f}   "
              f"{result['overfitting_gap']*100:>6.2f}%  {overfitting_icon}")
    
    # Analyse globale de l'overfitting
    print("\n" + "="*80)
    print("üîç ANALYSE DE L'OVERFITTING")
    print("="*80)
    
    overfitting_models = [name for name, res in results.items() if res['is_overfitting']]
    
    if overfitting_models:
        print(f"‚ö†Ô∏è  Mod√®les avec overfitting d√©tect√©: {', '.join(overfitting_models)}")
        print("üí° Recommandations:")
        print("   - Augmenter les donn√©es d'entra√Ænement")
        print("   - R√©duire la complexit√© du mod√®le")
        print("   - Utiliser plus de r√©gularisation")
    else:
        print("‚úÖ Aucun overfitting d√©tect√© sur aucun mod√®le!")
        print("üëç Tous les mod√®les g√©n√©ralisent bien aux nouvelles donn√©es")
    
    # Sauvegarder le meilleur mod√®le
    print("\n" + "="*80)
    print(f"üèÜ MEILLEUR MOD√àLE: {best_model_name}")
    print(f"üéØ Pr√©cision TEST: {best_accuracy * 100:.2f}%")
    print(f"üìä √âcart Train-Test: {results[best_model_name]['overfitting_gap'] * 100:.2f}%")
    
    if results[best_model_name]['is_overfitting']:
        print(f"‚ö†Ô∏è  Attention: Ce mod√®le pr√©sente de l'overfitting")
    else:
        print(f"‚úÖ Ce mod√®le g√©n√©ralise bien (pas d'overfitting)")
    
    if 'roc_auc_micro' in results[best_model_name]:
        print(f"üìà AUC Micro-Average: {results[best_model_name]['roc_auc_micro']:.3f}")
    
    print("="*80)
    
    print("\n Sauvegarde du meilleur mod√®le...")
    os.makedirs("ml", exist_ok=True)
    
    joblib.dump(best_model, "ml/model.pkl")
    joblib.dump(vectorizer, "ml/vectorizer.pkl")
    
    # Sauvegarder aussi les infos du mod√®le
    model_info = {
        'model_name': best_model_name,
        'accuracy': best_accuracy,
        'cv_mean': results[best_model_name]['cv_mean'],
        'cv_std': results[best_model_name]['cv_std'],
        'train_time': results[best_model_name]['train_time']
    }
    joblib.dump(model_info, "ml/model_info.pkl")
    
    print("‚úÖ Meilleur mod√®le sauvegard√© dans ml/model.pkl")
    print("‚úÖ Vectorizer sauvegard√© dans ml/vectorizer.pkl")
    print("‚úÖ Informations du mod√®le sauvegard√©es dans ml/model_info.pkl")
    
    # Tester quelques pr√©dictions
    print("\nüß™ Test de quelques pr√©dictions:")
    test_examples = [
        "Facture num√©ro 789 montant total 500 euros TVA incluse",
        "Exp√©rience professionnelle ing√©nieur Python comp√©tences machine learning",
        "Contrat de travail CDI salaire mensuel clause de confidentialit√©",
        "Lettre de motivation candidature poste d√©veloppeur",
        "Article de blog sur les nouvelles technologies"
    ]
    
    for example in test_examples:
        example_tfidf = vectorizer.transform([example])
        prediction = model.predict(example_tfidf)[0]
        probas = model.predict_proba(example_tfidf)[0]
        confidence = max(probas)
        
        print(f"\nüìÑ Texte: {example[:60]}...")
        print(f"   ‚Üí Cat√©gorie: {prediction} (confiance: {confidence * 100:.1f}%)")
    
    print("\n‚úÖ Entra√Ænement termin√© avec succ√®s!")
    print("üöÄ Le mod√®le est pr√™t √† √™tre utilis√© par l'API")

if __name__ == "__main__":
    train_model()
